{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01c: Transformers - (Visual) Attention is all you need\n",
    "\n",
    "As you almost certainly already know, attention revolutionized NLP. And while attention is primarily known for its impact in the NLP community, it can also be used in computer vision tasks.\n",
    "\n",
    "## Visual Attention: Spatial Transformer Networks\n",
    "\n",
    "An early application of attention in computer vision was the [spatial transformer network (STN)](https://arxiv.org/abs/1506.02025), which generalize differentiable attention to any spatial transformation. STNs allow a neural network to learn to perform spatial transformations on the input image in order to enhance the geometric invariance of the model. For example, it can crop a region of interest, scale and correct the orientation of an image. This can be useful because, as you know from the previous notebook, CNNs are not invariant to rotation and scale and more general affine transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
